{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_learning_Lab1",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krbe9202/Deep-learning/blob/master/Deep_learning_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vJW_un-sgiE",
        "colab_type": "text"
      },
      "source": [
        "## Deep learning HT19 - Laboration 1\n",
        "\n",
        "Laboration 1 aims to create a Convolutional Neural Network (CNN) model to classify the images in the MNIST fashion data set. \n",
        "\n",
        "The data set consists of 70 000 grayscale images, with resolution 28x28 pixels, where of 60 000 are training images and 10 000 are test images. There are 10 categories of clothing including shirts, trousers, pullovers, dresses, coats, bags and different types of shoes. \n",
        "\n",
        "I have segmented the assignment and each segment includes a brief explanation of the code. The results and discussion are presented at the end. Let's start by loading the MNIST fashion data and pre-processing it a bit.   \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDfffrHpwq2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.compat.v1.enable_eager_execution()  # if using Model subclassing\n",
        "print(tf.__version__)\n",
        "print(tf.executing_eagerly())\n",
        "\n",
        "mnist_fashion = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images,train_labels),(test_images,test_labels) = mnist_fashion.load_data()\n",
        "\n",
        "# the 10 classes of clothing, used for plotting\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# scale values to range from 0 to 1\n",
        "train_images, test_images = (train_images / 255.0), (test_images / 255.0)\n",
        "# one hot encode target column\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# add a channels dimension\n",
        "train_images = train_images[...,tf.newaxis]\n",
        "test_images = test_images[...,tf.newaxis]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bWGoSF5whjW",
        "colab_type": "text"
      },
      "source": [
        "**Data augmentation**  \n",
        "\n",
        "The idea behind data augmentation is creating more training examples by taking the existing ones and performing image transformations such as rotation, shearing, translation, zooming and so on. Here, this is done by using ImageDataGenerator. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MxYIUDUxKXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_aug = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "train_generator=data_aug.flow(\n",
        "    train_images, \n",
        "    train_labels, \n",
        "    batch_size=32,\n",
        "    shuffle=True)\n",
        "\n",
        "# inspect augmented images\n",
        "plt.imshow(np.squeeze(train_generator[0][0][0]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68JdzWBNxDxB",
        "colab_type": "text"
      },
      "source": [
        "**Function to build models**\n",
        "\n",
        "This function builds the CNN Sequential model and it utilises max pooling and, if desired, droupout and batch normalization. Max pooling is a sample-based discretization process where the input is down-sampled, reducing its dimensionality. This helps with over-fitting by providing an abstracted form of representation as well as reducing computational costs by decreasing the amount of lernable parameters. It is usually done by applying a max filter to subregions of the initial representation. Dropout is a regularization method and can be applied to both input layers and the hidden layers. It randomly masks the outputs of a fraction of units from a layer by setting their output to zero. Batch normalization is technique, usually applied for deep or very deep NNs, that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks. \n",
        "\n",
        "\n",
        "*Required arguments for the function*\n",
        "\n",
        "**k_size:** tuple, specifying the kernel size. \n",
        "\n",
        "**stride:** int, specifying the pace at which the kernel is transversed over the image. \n",
        "\n",
        "**use_bn:** boolean, specifying if batch normalization should be used or not. \n",
        "\n",
        "**use_dp:** boolean, specifying if drop out should be used or not. \n",
        "\n",
        "**num_classes:** int, the number of classes we want to classify. In our case, this is 10. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP7qnBYfT67p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Activation, BatchNormalization, InputLayer\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "in_shape=train_images[0,:].shape\n",
        "\n",
        "def build(k_size:tuple, stride:int,\n",
        "          use_bn:bool, use_dp:bool, num_classes:int):\n",
        "  # initiate model\n",
        "  model=Sequential()\n",
        "  # add first convolutional layer\n",
        "  model.add(Conv2D(32, k_size, stride, padding='same', activation='relu', \n",
        "                   kernel_initializer='he_uniform',\n",
        "                   input_shape=in_shape))\n",
        "  if use_bn:\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "  # add second convolutional layer\n",
        "  model.add(Conv2D(64, k_size, stride, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  if use_dp: \n",
        "    model.add(Dropout(0.25))\n",
        "  # add third convolutional layer\n",
        "  model.add(Conv2D(64, k_size, stride, padding='same', activation='relu'))\n",
        "  # add fully connected layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  if use_dp: \n",
        "    model.add(Dropout(0.5))\n",
        "  # add output layer with 10 nodes for classification\n",
        "  model.add(Dense(10, activation=\"softmax\"))\n",
        "  \n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-4), \n",
        "              metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F78cdG4HOAb7"
      },
      "source": [
        "**Train model and evaluate model performance**\n",
        "\n",
        "I performed some experiments with kernel size, stride, batch size (i.e. the total number of images passed to the model per iteration), dropout (fraction and position) and batch normalization (position) (see Results and Discussion). \n",
        "\n",
        "An alternative could be to use cross validation and split the training data into two (training and validation), and then use the external test data for performance evaluation. However, hyperparameter optimization using cross validation is not really optimal for CNNs as they typically  have a large amount of parameters (e.g. more than one million). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gjjwu_m5doiQ",
        "colab": {}
      },
      "source": [
        "\n",
        "BS = 64\n",
        "EPOCHS=50\n",
        "\n",
        "# train a model \n",
        "def train(model,train_x, train_y, test_x, test_y):\n",
        "    scores, histories = list(), list()\n",
        "\n",
        "    # either use data augmentation or not for training\n",
        "\n",
        "    # history = model.fit(train_x, train_y, epochs=EPOCHS, batch_size=BS, \n",
        "    #                     validation_data=(test_x, test_y), verbose=1)\n",
        "   \n",
        "    history = model.fit_generator(train_generator, \n",
        "                                  validation_data=(test_x, test_y), \n",
        "                                  steps_per_epoch= len(train_x) / BS, epochs=EPOCHS)\n",
        "    \n",
        "    # evaluate model\n",
        "    _, acc = model.evaluate(test_x, test_y, verbose=0)\n",
        "    print('Accuracy on test data: %.3f' % (acc * 100.0))\n",
        "    # append scores\n",
        "    scores.append(acc)\n",
        "    histories.append(history)\n",
        "    return scores, histories\n",
        "\n",
        "# build model\n",
        "model = build((7,7),1,False,False,10)\n",
        "\n",
        "scores, histories = train(model,train_images, train_labels, \n",
        "                          test_images, test_labels)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e6RgrSyrlCo8"
      },
      "source": [
        "**Plot models**\n",
        "\n",
        "Here I define some functions for plotting trend for model- histories and losses, and predicted images with percentage of accurate predictions versus actual labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2d3N7w3poHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # plot diagnostic learning curves\n",
        "def diagnostics(histories):\n",
        "  for i in range(len(histories)):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(histories[i].history['acc'], color='blue', label='train')\n",
        "    plt.plot(histories[i].history['val_acc'], color='orange', label='test')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# plot model\n",
        "diagnostics(histories)\n",
        "\n",
        "\n",
        "# function that calculates the percent of correctly predicted labels compared to \n",
        "# true labels. The output is an image with its corresponding predicted label,\n",
        "# the probability for the predicted label and true label. The color of the xlabel \n",
        "# indicates if the correction is correct (this doesn't show in the actual image, \n",
        "# and I don't know why...)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "def image_percent(i, predictions, true_label, img): \n",
        "  predictions,true_label,img=predictions,true_label[i],img[i].reshape(28,28)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  plt.imshow(img,cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions)\n",
        "\n",
        "  if predicted_label == np.argmax(true_label):\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red' \n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions),\n",
        "                                class_names[int(np.argmax(true_label))],\n",
        "                                color=color))\n",
        "\n",
        "# plot the first 25 images\n",
        "nrows=5\n",
        "ncols=5\n",
        "plt.figure(figsize=(2.5*nrows,2.5*ncols))\n",
        "for i in range(1,nrows*ncols+1):\n",
        "    plt.subplot(nrows,ncols,i)\n",
        "    image_percent(i,predictions[i],test_labels,test_images)\n",
        "plt.show()\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM3-3AHEHOxC",
        "colab_type": "text"
      },
      "source": [
        "**Transfer learning**\n",
        "\n",
        "Transfer learning according to Ian Goodfellow et al. (2016) in *Deep learning* it can be defined as:   \n",
        "\n",
        "\"*Situation where what has been learned in one setting is exploited to improve generalization in another setting.*\"\n",
        "\n",
        "Basically, it means that ... \n",
        "\n",
        "So in transfer learning you can leverage knowledge (features, weights etc) from previously trained models for training newer models and this offer some advantages, for example, it can be useful for dealing with problems such as having less data for the newer task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rJ6lr40HTed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import cv2\n",
        "\n",
        "\n",
        "def resize(images): \n",
        "  scale = 1.15  # images must 32 px or larger to be used as input for vgg16\n",
        "  resized_train = []\n",
        "  for i in range(len(images)): \n",
        "    # make images 3-channeled\n",
        "    train_temp = np.squeeze(np.stack((images[i],) * 3, -1))\n",
        "    width = int(train_temp.shape[1] * scale)\n",
        "    height = int(train_temp.shape[0] * scale)\n",
        "    dim = (width, height)\n",
        "    # resize image\n",
        "    resized_train.append(cv2.resize(train_temp, dim, \n",
        "                                    interpolation = cv2.INTER_AREA))\n",
        "  return resized_train \n",
        "\n",
        "# resize training and testing images\n",
        "resize_train = resize(train_images)\n",
        "resize_val = resize(test_images)\n",
        "\n",
        "# check size\n",
        "print(np.array(resize_train).shape) \n",
        "print(np.array(resize_val).shape) \n",
        "\n",
        "# get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "model_vgg16_conv.summary()\n",
        "\n",
        "# freeze the VGG16 parameters \n",
        "model_vgg16_conv.trainable = False\n",
        "for layer in model_vgg16_conv.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# create your own input format \n",
        "input = Input(shape=(32,32,3),name = 'image_input')\n",
        "\n",
        "# use the generated model \n",
        "output_vgg16_conv = model_vgg16_conv(input)\n",
        "\n",
        "# add the fully-connected layers \n",
        "x = Flatten(name='flatten')(output_vgg16_conv)\n",
        "x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "x = Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "# create your own model \n",
        "my_vgg_model = Model(inputs=input, outputs=x)\n",
        "my_vgg_model.summary()\n",
        "\n",
        "\n",
        "my_vgg_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "BS = 100\n",
        "EPOCHS=100\n",
        "\n",
        "history = my_vgg_model.fit(x=np.array(resize_train), y=train_labels,\n",
        "                    validation_data=(np.array(resize_val), test_labels),\n",
        "                    batch_size=BS,\n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=1)\n",
        "\n",
        "def plot_models(histories:list, key:str): \n",
        "  plt.figure(figsize=(16,10))\n",
        "\n",
        "  for name, history in histories: \n",
        "    val = plt.plot(history.epoch, history.history['val_'+key], '--',\n",
        "                   label=name.title()+' validation')\n",
        "    plt.plot(history.epoch,history.history[key], color=val[0].get_color(), \n",
        "             label=name.title()+' train')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key)\n",
        "  plt.legend()\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "  plt.show()\n",
        "\n",
        "plot_models([('Model', history)],  \n",
        "               \"acc\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTBCTDp2BK2W",
        "colab_type": "text"
      },
      "source": [
        "## Results \n",
        "\n",
        "(It should work to click on the links of the images. If not, I'm sorry)\n",
        "\n",
        "### Experiment 1 - Changing kernel size and stride\n",
        "\n",
        "(all tests below implement drop out and batch size = 64) \n",
        "\n",
        "\n",
        "\n",
        "*   kernel size (3,3) & stride 1\n",
        "[Plot showing histories and cross entropy losses (train, validation) for the own built CNN model using kernel size 3 and stride 1](https://drive.google.com/file/d/1eMzLZgrY-gslcO4LNlTQIYKNRaDxnnhm/view?usp=sharing)\n",
        "*   kernel size (7,7) &  stride 1\n",
        "[Plot showing histories and cross entropy losses (train, validation) for the own built CNN model using kernel size 7 and stride 1](https://drive.google.com/file/d/1rKrsy12p_01vehitCKJdXOxlsr4bqLFj/view?usp=sharing)\n",
        "*   kernel size (7,7) &  stride 2 \n",
        "[Plot showing histories and cross entropy losses (train, validation) for the own built CNN model using kernel size 7 and stride 2](https://drive.google.com/file/d/1T6bwl41u8QOTf9uVnLSUEomERpTQmUiH/view?usp=sharing)\n",
        "\n",
        "Image showing the first 25 classified image with accurate label prediction rates respectively (looked sort of similiar for all runs tbh). \n",
        "[25 classified images with corresponding correctly predicted label rates](https://drive.google.com/file/d/1wNU7OfaXYrZFdFNNfg21LCxZzm3ipdnK/view?usp=sharing)\n",
        "\n",
        "### Experiment 2 - Data augmentation \n",
        "\n",
        "[Plot showing histories and cross entropy losses for run not utilizing data augmentation. (k_size = (7,7), stride = 1)](https://drive.google.com/file/d/152YqgTMpjseH7T7liYqRfIgyXXqk_pth/view?usp=sharing)\n",
        "\n",
        "Image in bullet point 2 in above section shows result using data augmentation.  \n",
        "\n",
        "### Experiment 3 - Regularization methods\n",
        "\n",
        "*   Drop out\n",
        "[Image of histories, losses for train, val using no data augmentation and no drop out](https://drive.google.com/file/d/1niMjDEeiMOlsNxWkZ7eC4dBMvkBfukOy/view?usp=sharing)\n",
        "*   Batch normalization \n",
        "\n",
        "\n",
        "\n",
        "### Experiment 4 - Transfer learning\n",
        "\n",
        "[Plot showing histories (train, validation) for the model using transfer learning](https://drive.google.com/file/d/149h3fTEP9h0B_AcWQyWz4Mcv-JQr0xya/view?usp=sharing)\n",
        "\n",
        "## Discussion\n",
        "\n",
        "*Playing around with kernel size & stride*\n",
        "\n",
        "Overall, the experiments show that increasing stride when using a larger filter size decreases accuracy for both training and test data. This makes sense considering that we are acutally retrieving less information from the images along the sequence as we are jumping twice the distance (in the case for bullet point 2), hence left with a smaller (less pixels) compared to when stride 1 is used. Similarly trend is seen when increasing kernel size, even when keeping them relatively small, but keeping in mind that our images have really low resolution. \n",
        "\n",
        "*Data augmentation*\n",
        "\n",
        "In the beginning, using data augmentation heavily decreased accuracy on validation data. I first believed this was because my model didn't have enough capacity, so I added another convolutional layer. However, this did not solve the issue. After I inspected the generated augmentated images, I realized that they were the issue. The images had gotten way too distorted to a point were the clothing items were not recognizable anymore. Therefore, I adjusted the parameters in ImageDataGenerator, and the accuracy on validation data increased from ~10-20% to ~92%-93% after 50 epochs, a clear improvement! Crap in, crap out, as they say. \n",
        "\n",
        "The plots in the results section also show that accuracy for validation data declines (and cross entropy loss inclines) compared to training when data augmentation is not used. It rather looks like the model starts to overfit the training data, possbily because it has too many parameters in relation to amount of input data. Therefore increasing the amount of input data can aid in reducing overfitting. Although, I wonder if I changed the parameters so much that the augmented images barely look different to the original images, and therefore I'm essentially just feeding almost copies of the same images to the model. \n",
        "\n",
        "*Regularization*\n",
        "\n",
        "The plot in the result section where drop out is not used show a large discrepancy between train and validation trend for both loss and accuracy, indicating that drop out actually helps with not over-fitting the model (compare it to the plot in bullet point 2 under Experiment 1). Further work includes changing the fraction and position of the drop out layers to see if that makes any difference. \n",
        "\n",
        "\n",
        "*Transfer learning*\n",
        "\n",
        "The plot indicates over-fitting as we can see the accuracy for the training data steadily increasing while the accuracy for the test data plateaus at ~88% after around 30 epochs. Seeing as this model have a large number of learnable parameters (18,923,530 to be exact), this result makes sense, we have too much capacity for the input data. It would be interesting to continue working on this model and implementing, for example, data augmentation and drop out, to see if it helps with reducing over-fitting. Also, I don't know if it makes a difference using different pre-trained models, I just choose VGG16 because I found many tutorials using it.  \n",
        "\n"
      ]
    }
  ]
}